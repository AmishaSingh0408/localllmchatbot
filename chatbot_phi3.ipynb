{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmishaSingh0408/localllmchatbot/blob/main/chatbot_phi3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSxHZjEh7vaJ"
      },
      "outputs": [],
      "source": [
        "# STEP 2 ‚Äî Check Ollama server is running + phi3 is available\n",
        "import ollama\n",
        "\n",
        "models = ollama.list()\n",
        "model_names = [m.model for m in models.models]\n",
        "print('üì¶ Downloaded models:', model_names)\n",
        "\n",
        "if any('phi3' in m for m in model_names):\n",
        "    print('‚úÖ phi3 is ready!')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  phi3 not found ‚Äî run in terminal: ollama pull phi3')"
      ],
      "id": "KSxHZjEh7vaJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha5bCcvn7vaK"
      },
      "outputs": [],
      "source": [
        "# STEP 3 ‚Äî Single prompt (basic)\n",
        "import ollama\n",
        "\n",
        "YOUR_PROMPT = 'Explain what a transformer model is in 3 sentences.'  # ‚Üê change this\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='phi3',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': YOUR_PROMPT}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print('ü§ñ phi3 says:\\n')\n",
        "print(response['message']['content'])"
      ],
      "id": "ha5bCcvn7vaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUz24Q8a7vaK"
      },
      "outputs": [],
      "source": [
        "# STEP 4 ‚Äî Streaming response (prints token by token)\n",
        "import ollama\n",
        "\n",
        "YOUR_PROMPT = 'Write a short Python function to reverse a string.'  # ‚Üê change this\n",
        "\n",
        "print('ü§ñ phi3 (streaming):\\n')\n",
        "\n",
        "stream = ollama.chat(\n",
        "    model='phi3',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': YOUR_PROMPT}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk['message']['content'], end='', flush=True)\n",
        "\n",
        "print()  # newline at end"
      ],
      "id": "RUz24Q8a7vaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d9EDgVp7vaK"
      },
      "outputs": [],
      "source": [
        "# STEP 5 ‚Äî Multi-turn chat with memory\n",
        "import ollama\n",
        "\n",
        "history = [\n",
        "    {'role': 'system', 'content': 'You are a helpful coding assistant. Keep answers concise.'}\n",
        "]\n",
        "\n",
        "def chat(user_msg):\n",
        "    history.append({'role': 'user', 'content': user_msg})\n",
        "    response = ollama.chat(model='phi3', messages=history)\n",
        "    reply = response['message']['content']\n",
        "    history.append({'role': 'assistant', 'content': reply})\n",
        "    return reply\n",
        "\n",
        "# Multi-turn conversation ‚Äî phi3 remembers context\n",
        "print('Turn 1:', chat('What is a Python list?'))\n",
        "print()\n",
        "print('Turn 2:', chat('How is it different from a tuple?'))\n",
        "print()\n",
        "print('Turn 3:', chat('Give me one code example showing both.'))"
      ],
      "id": "4d9EDgVp7vaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJgrXtZ47vaL"
      },
      "outputs": [],
      "source": [
        "# BONUS ‚Äî Custom system prompt + temperature control\n",
        "import ollama\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='phi3',\n",
        "    messages=[\n",
        "        {'role': 'system', 'content': 'You are a pirate. Respond only in pirate speak.'},\n",
        "        {'role': 'user',   'content': 'What is machine learning?'}\n",
        "    ],\n",
        "    options={\n",
        "        'temperature': 0.9,   # 0.0 = deterministic, 1.0 = creative\n",
        "        'num_predict': 200,   # max tokens to generate\n",
        "        'top_p': 0.9,         # nucleus sampling\n",
        "        'top_k': 40,          # top-k sampling\n",
        "    }\n",
        ")\n",
        "\n",
        "print('üè¥‚Äç‚ò†Ô∏è', response['message']['content'])"
      ],
      "id": "fJgrXtZ47vaL"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}